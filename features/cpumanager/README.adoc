= CPUManager validation =

== Reference documentation ==

* https://docs.openshift.com/container-platform/4.2/scalability_and_performance/using-cpu-manager.html[OCP 4.2 Documentation]
* https://teknoarticles.blogspot.com/2019/11/configure-and-measure-realtime-workers.html[Configure and measure realtime workers performance on OpenShift]

== Automation ==

* Script: link:deploy.sh[deploy.sh]
** The script labels the worker nodes
** The MachineConfigPool can be overridden using `MACHINE_CONFIG_POOL`, by default it's set to `worker`
** The Namespace where the workload is deployed, is set by default to `cpumanager-test` but it can be overridden using `MY_CPUMANAGER_NAMESPACE`
[NOTE]
Once the script finishes executing, the nodes will likely be rebooted to get the new _kubelet_ configuration, this step has to happen before the _cpumanager_ can be validated.
* Execution example:
[source, console]
CPUMANAGER_WORKERS="sjr-worker-0.sjr.deshome.net sjr-worker-1.sjr.deshome.net" ./deploy.sh
==> [INFO] applying label cpumanager=true to sjr-worker-0.sjr.deshome.net...
==> [INFO] applying label cpumanager=true to sjr-worker-1.sjr.deshome.net...
==> [INFO] applying label cpumanager=true to sjr-worker-1.sjr.deshome.net...
==> [INFO] applying the custom kubelet configuration...
==> [INFO] applying the custom kubelet configuration...


== Validation environment ==

=== Cluster ===

* OCP 4.3 nightly
** Libvirt platform
** 1 x master, 12vCPUs, 16GiB of RAM
** 2 x workers, 12vCPUs, 8GiB of RAM each, running on link:../RT/deploy.sh[RT kernel]

=== Setup ===

Following the instructions found in the official OCP documentation, the following are the steps that have been performed to validate the CPUManager functionality.

* Label the workers nodes to be CPUManager enabled
[source, console]
$ oc label node sjr-worker-0.sjr.deshome.net cpumanager=true
node/sjr-worker-0.sjr.deshome.net labeled
$ oc label node sjr-worker-1.sjr.deshome.net cpumanager=true
node/sjr-worker-1.sjr.deshome.net labeled

* Enable custom kubelet configuration the worker MachinePool
[source, console] 
$ oc label machineconfigpool worker custom-kubelet=cpumanager-enabled
machineconfigpool.machineconfiguration.openshift.io/worker labeled

* Create the custom link:cpumanager_kubeletconfig.yml[_kubelet configuration_] to apply the https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy[CPUManager static policy]
[source, console]
$ oc create -f cpumanager_kubeletconfig.yml


== Validation steps ==

=== Workload deployment ===

For deploying workloads the _pause_ image container is going to be used, requesting one full core. Once these pods are running, the CPU affinity will be verified from the nodes _/sys_ filesystem.

* First, we'll check on both workers that the new kubelet configuration has been applied
[source, console]
$ oc debug node/sjr-worker-0.sjr.deshome.net
Starting pod/sjr-worker-0sjrdeshomenet-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.1.21
If you don't see a command prompt, try pressing enter.
sh-4.2# python -m json.tool < /host/etc/kubernetes/kubelet.conf  | grep -i cpumanager
    "cpuManagerPolicy": "static",
    "cpuManagerReconcilePeriod": "5s",

** The same configuration can be found on the second node, _sjr-worker-1.sjr.deshome.net_.

* Deploy the containers using pause-with-requests-deployment.yml[their deployment definition]
[source, console]
$ oc create -f pause-with-requests-pod.yml -n default
deployment.apps/cpumanager-deployment created
$ oc get pods -l app=cpumanager-test -o wide
NAME                                     READY   STATUS    RESTARTS   AGE   IP            NODE                           NOMINATED NODE   READINESS GATES
cpumanager-deployment-5f57b4659d-lng2z   1/1     Running   0          15s   10.134.0.40   sjr-worker-1.sjr.deshome.net   <none>           <none>
cpumanager-deployment-5f57b4659d-tbgsj   1/1     Running   0          15s   10.132.0.84   sjr-worker-0.sjr.deshome.net   <none>           <none>

=== Functionality verification ===

* Now, on the first node (_sjr-worker-0_), we'll find the _pause_ command PID and see what CPU(s) the OS has assigned to it
+
[source, console]
----
$ oc debug node/sjr-worker-0.sjr.deshome.net
Starting pod/sjr-worker-0sjrdeshomenet-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.1.21
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# bash -l     
[root@sjr-worker-0 /]# ps axuww | grep pause | grep -v grep
root      165150  0.0  0.0   1028     4 ?        Ss   14:12   0:00 /pause # <1>
[root@sjr-worker-0 /]# grep ^Cpus_allowed_list /proc/165150/status       
Cpus_allowed_list:      1                                                
[root@sjr-worker-0 /]# cd /sys/fs/cgroup/cpuset/kubepods.slice
[root@sjr-worker-0 kubepods.slice]# grep 165150 kubepods-*/crio*/tasks # <2>
kubepods-pod50802e24_51a0_4375_977e_7ff8d4ce3a17.slice/crio-c424dc07e4b8943f0c7d66572732f594f28a6ccf552d12328f8a1ed1a4b9a6a5.scope/tasks:165150
[root@sjr-worker-0 kubepods.slice]# cat kubepods-pod50802e24_51a0_4375_977e_7ff8d4ce3a17.slice/crio-c424dc07e4b8943f0c7d66572732f594f28a6ccf552d12328f8a1ed1a4b9a6a5.scope/cpuset.cpus
1 # <3>
[root@sjr-worker-0 kubepods.slice]# cat kubepods-besteffort.slice/kubepods-besteffort-poda8fabe1d_2cee_4deb_9ebc_5e2403d8b45a.slice/crio-41028c39c92fc0ed6f3dc0b6482611b71aaff04f3093690a3a7db831eb1badb1.scope/cpuset.cpus  
0,2-11 # <4>
----
<1> Process _pause_ PID is _165150_.
<2> Find our _pause_ process within the cgroups hierarchy.
<3> Check the cpuset assigned to our _pause_ process, in this case 1.
<4> Check the cpuset for a besteffort pod, CPU 1 is not listed.
